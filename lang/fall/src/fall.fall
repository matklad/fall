// This is the grammar for the Fall parser generator input language.
// It is itself written in Fall and serves as the primary
// tutorial / documentation. To get syntax highlighting for this file,
// use VS Code plugin from the `code` subdirectory.

// The syntax is Rust inspired.

// In Fall, lexical and syntactical analyses are conducted in separate
// phases, because it's in theory much simpler to make lexical analysis
// incremental. Nevertheless, both lexical and syntactical grammars
// are specified in a single file.

// Lexical tokens are specified inside `tokenizer` block.
tokenizer {
  // Each token consists of a name and a regular expression pattern,
  // optionally preceded by attributes.

  // The `#[skip]` attribute is used for whitespace and comments.
  // The parser does not see `#[skip]` tokens, they are attached
  // to the relevant nodes automatically.
  #[skip] whitespace r"\s+"
  #[skip] eol_comment r"//.*"

  // Contextual attribute is required to support contextual keywords.
  // The lexer ignores contextual token and, for example, parses `node`
  // as an identifier. The parser however will remap identifier to the
  // node keyword. You typical don't need `#[contextual]`, but we use
  // it in Fall to avoid clashes with keywords in the target language.
  #[contextual] node 'node'
  #[contextual] class 'class'
  #[contextual] tokenizer 'tokenizer'
  #[contextual] rule 'rule'
  #[contextual] verbatim 'verbatim'
  #[contextual] ast 'ast'
  #[contextual] pub 'pub'
  #[contextual] test 'test'

  // If you use `''` to specify the pattern, it will be interpreted "as is",
  // and not as a regular expression.
  eq '='
  pipe '|'
  star '*'
  question '?'
  dot '.'
  comma ','
  hash '#'
  l_curly '{'
  r_curly '}'
  l_square '['
  r_square ']'
  l_angle '<'
  r_angle '>'
  l_paren '('
  r_paren ')'

  number r"\d+"
  // Raw literals like `r###"contains "a string" inside "###` are supported.
  simple_string r#"'([^'\\]|\\.)*'"#

  // Besides the name and the pattern, you can optionally provide
  // a name of a Rust function which will be used to parse the token.
  // See `verbatim` block below.
  hash_string r#"r#*""# 'parse_raw_string'
  ident r"\w+"
}

// After the `tokenizer` block syntactical grammar rules follows.

// The first rule is special: it is implicitly used as an entry
// point for the grammar.
pub rule fall_file {
  <with_skip file_form_first file_form>*
}

rule file_form_first {
  'tokenizer' | 'pub' | 'rule' | '#' | 'verbatim' | 'ast' | 'test'
}

rule file_form {
  tokenizer_def
| syn_rule
| verbatim_def
| ast_def
| test_def
}

// Each rule has the following pattern:
pub rule syn_rule {
  // Optional `#[foo]` style attributes
  attributes?

  // Each `pub` rule creates a separate node type.
  // That is, each `pub` rule has a dedicated node in the resulting
  // parse tree. In theory, you can inline all non-`pub` rules and
  // get functionally the same parser.
  'pub'?

  'rule' <commit>

  // Each rule has a name. `pub` rule `foo_bar` will create a
  // `FOO_BAR` NodeType constant.
  ident

  parameters?

  // The body of the rule is enclosed in `{`,
  // see expression grammar below for details.
  <layer 
    { '{' in_braces '}' | '{' {<not file_form_first> <any>}* }
    block_expr
  >
}


pub rule parameters {
  '(' <layer in_parens <comma_list parameter>> ')'
}

pub rule parameter { ident }

test r"
rule foo {
  'struct' type_parameters? '{' 
    field_def*
  '}'
}
"

#[pratt]
rule expr { 
  call_expr | ref_expr | block_expr 
| opt_expr | rep_expr
}

// The simplest rule is just a reference to another rule or token.
#[atom]
pub rule ref_expr {
  ident         // <- this is the reference to the `ident` token
| simple_string // To refer to tokens like keywords or operators, use `'+'` syntax
}

// You can write `A B` to match A and then B.
pub rule seq_expr { expr* }

// Or you can write `{ A | B }` to match either of A or B.
// Note that B will be tried only if A didn't match.
// Corollary: if A can match an empty sting (e.g. it is optional),
// B will never be matched!
#[atom]
pub rule block_expr {
  // Note how we refer to `pipe` token using a `simple_string` `'|'`.
  // We use outer `{}` purely for grouping here. There's no `()`.
  <block { seq_expr? {'|' seq_expr}* }>
}

#[postfix] 
pub rule opt_expr {
  expr '?'
}

#[postfix] 
pub rule rep_expr {
  expr '*'
}

// Another, very powerful construct is to invoke a function
// using other expressions as arguments.
// Some predefined functions are
//
//   * `<opt arg>`: match `arg` or an empty string,
//   * `<rep arg>`: match zero, one or more repetitions of `arg`.
//
// There are also user-defined functions, such as `block` from above.
#[atom]
pub rule call_expr { '<' ident expr* '>' }


pub rule tokenizer_def {
  // Commit is a special special rule. It allows parser to produce
  // a node even if some input is missing.
  // In this case, as long as parser passes `tokenizer` keyword,
  // it will produce `tokenizer_def` node, even if it is not followed
  // by the `{}` block.

  // If you are using VS Code plugin, the best way to understand commit
  // is to invoke **Syntax Tree* VS Code action to view the syntax
  // tree of the current file and type garbage before and after various
  // commit points.
  'tokenizer' <commit>
  <block lex_rule*>
}

// Now then you know how rules work, can you see how `lex_rule` rule
// allows us to parse `tokenizer` block? What will happen if you
// omit the required regular expression pattern?
pub rule lex_rule {
  attributes? ident <commit> string string?
}

pub rule test_def { 'test' <commit> hash_string }


pub rule attributes {
    '#' '[' 
       <layer in_bracks <comma_list attribute>>
    ']'
}

pub rule attribute {
  ident {'(' attribute_value ')'}?
}

pub rule attribute_value {
  number | ident
}


pub rule string { simple_string | hash_string }

rule comma_list(el) {
  {el <commit> {<eof> | ','}}*
}

rule block(body) { '{' <commit> <layer in_braces body> '}'}

rule in_braces { <balanced '{' '}'> }
rule in_parens { <balanced '(' ')'> }
rule in_bracks { <balanced '[' ']'> }

rule balanced(bra, ket) {
  { bra <balanced bra ket> ket 
  | {<not ket> <any>}
  }*
}

// Verbatim block is pasted as is into the generated parser.
// You can use it to provide custom tokenizer rules.
// For example, the grammar for rust-style raw string literals
// is not regular (in fact, it is not even context free! o_O),
// so we use a hand written `parse_raw_string` function to deal
// with them.
pub rule verbatim_def { 'verbatim' <commit> hash_string }

verbatim r#########"

fn parse_raw_string(s: &str) -> Option<usize> {
    let quote_start = s.find('"').unwrap();
    // Who needs more then 25 hashes anyway? :)
    let q_hashes = concat!('"', "######", "######", "######", "######", "######");
    let closing = &q_hashes[..quote_start];
    s[quote_start + 1..].find(closing).map(|i| i + quote_start + 1 + closing.len())
}

"#########

pub rule ast_def {
  'ast' <commit> '{'
    <with_skip
      {'node' | 'class'}
      {ast_node_def | ast_class_def}>*
  '}'
}

pub rule ast_node_def {
  'node' <commit> ident <block method_def*>
}

pub rule ast_class_def {
  'class' <commit> ident <block ident*>
}

pub rule method_def { ident ast_selector }
pub rule ast_selector { ident ast_selector_suffix? }
rule ast_selector_suffix { '?' '.' ident | '.' ident | '*' | '?' }

// The last (optional) block in the file is AST definition.
// Parser produces untyped concrete syntax tree, with all comments
// and whitespaces. However to add semantics to the language, it is
// convenient to get a typed view into the syntax tree structure.
// ast block does exactly that! It does not change the parser in any way,
// but it generates a bunch of structs with convenient methods.
ast {

  // This definition will result in the following struct:
  //
  //  struct FallFile { ... }
  //  impl FallFile {
  //      pub fn tokenizer_def(&self) -> Option<TokenizerDef>
  //      pub fn syn_rules(&self) -> impl Iterator<Item=SynRule>
  //      pub fn verbatim_def(&self) -> Option<VerbatimDef>
  //      pub fn ast_def(&self) -> Option<AstDef>
  //      pub fn examples(&self) -> impl Iterator<Item=ExampleDef>
  //  }
  node fall_file {
    tokenizer_def tokenizer_def?
    syn_rules syn_rule*
    verbatim_def verbatim_def?
    ast_def ast_def?
    tests test_def*
  }

  node tokenizer_def {
    lex_rules lex_rule*
  }

  node lex_rule {
    attributes attributes?
    node_type ident.text
  }

  node syn_rule {
    attributes attributes?
    name_ident ident?
    name ident?.text
    body expr
    parameters parameters?
  }

  node parameters { 
    parameters parameter*
  }

  node parameter {
    name ident.text
  }

  node attributes {
    attributes attribute*
  }

  node attribute {
    name ident.text
    value attribute_value?
  }

  node attribute_value { }

  node verbatim_def { 
    literal_string hash_string.text
  }

  node ast_def {
    ast_nodes ast_node_def*
    ast_classes ast_class_def*
  }

  node ast_node_def {
    name_ident ident
    name ident.text
    methods method_def*
  }

  node ast_class_def {
    name_ident ident
  }

  node method_def {
    name ident.text
    selector ast_selector
  }

  node ast_selector {
    child ident.text
    optional question?
    many star?
    dot dot?
  }

  node test_def { 
    literal_string hash_string?.text
  }

  class expr {
    ref_expr call_expr block_expr
    opt_expr rep_expr
    seq_expr
  }

  node ref_expr {  }

  node call_expr {
    fn_name ident.text
    args expr*
  }

  node block_expr {
    alts expr*
  }

  node opt_expr { 
    expr expr
  }

  node rep_expr { 
    expr expr
  }

  node seq_expr {
    parts expr*
  }

}
